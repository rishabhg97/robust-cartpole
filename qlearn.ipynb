{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$$$ True\n",
      "Loading pretrained PPO Model from: ./models/ppo/ppo-cartpole.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishabhgarg/anaconda3/envs/robustness-cartpole/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import openai_cartpole\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn \n",
    "from stable_baselines3 import PPO  # Assuming stable_baselines3 is used for PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import os,argparse\n",
    "import tensorflow as tf\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Train a PPO agent with adversarial environment modifications.')\n",
    "parser.add_argument('--log', action='store_true', help='Enable logging to files')\n",
    "parsedargs = parser.parse_args(['--log'])\n",
    "\n",
    "logfile='./logs/training_logs.txt'\n",
    "log_dir='./logs'\n",
    "SEED=42\n",
    "\n",
    "def log_info(inpstr, *args):\n",
    "    message = inpstr.format(*args)\n",
    "    if parsedargs.log:\n",
    "        os.makedirs(os.path.dirname(logfile), exist_ok=True)\n",
    "        with open(logfile, 'a') as file:\n",
    "            file.write(f\"{message}\\n\")\n",
    "        print(message)\n",
    "    \n",
    "    \n",
    "    \n",
    "# Create the modified CartPole environment\n",
    "env = gym.make('openai_cartpole/ModifiedCartPole-v1', render_mode=\"human\")\n",
    "env.reset()\n",
    "#env=Monitor(env, log_dir, allow_early_resets=True)\n",
    "ppo_filename='./models/ppo/ppo-cartpole.zip'\n",
    "print(\"$$$$\",os.path.exists(ppo_filename))\n",
    "# Train a PPO agent on the cartpole with a fixed seed for reproducibility\n",
    "if os.path.exists(ppo_filename):\n",
    "    log_info(\"Loading pretrained PPO Model from: {}\", ppo_filename)\n",
    "    ppo_agent = PPO.load(ppo_filename)\n",
    "else:\n",
    "    log_info(\"Training PPO Model and saving: {}\", ppo_filename)\n",
    "    ppo_agent = PPO(\"MlpPolicy\", env, seed=SEED, verbose=1, tensorboard_log=\"./logs/ppo\")\n",
    "    ppo_agent.learn(total_timesteps=10000)\n",
    "    ppo_agent.save(ppo_filename)\n",
    "mean_reward, std_reward = evaluate_policy(ppo_agent, env, n_eval_episodes=100)\n",
    "print(f\"Eval reward: {mean_reward} (+/-{std_reward})\")\n",
    "# TODO: Check if the training done is reproducible\n",
    "\n",
    "# Keep the agent frozen throughout the adversary training \n",
    "# create ppo agent for the carpole agent, then make changes for each action chosen by Q space\n",
    "\n",
    "\n",
    "# Original Parameters\n",
    "gravity, masscart, masspole, length, force_mag = env.get_params()\n",
    "\n",
    "# Range of the delta changes in the parameters\n",
    "GRAVITY_MIN, GRAVITY_MAX = (gravity * 0.9, gravity * 1.1)\n",
    "MASSCART_MIN, MASSCART_MAX = (masscart * 0.9, masscart * 1.1)\n",
    "MASSPOLE_MIN, MASSPOLE_MAX = (masspole * 0.9, masspole * 1.1)\n",
    "LENGTH_MIN, LENGTH_MAX = (length * 0.9, length * 1.1)\n",
    "FORCE_MAG_MIN, FORCE_MAG_MAX = (force_mag * 0.9, force_mag * 1.1)\n",
    "\n",
    "\n",
    "\n",
    "# Define the action space for the adversary\n",
    "action_space = {\n",
    "    \"delta_gravity\": np.linspace(GRAVITY_MIN-gravity, GRAVITY_MAX-gravity, 3),\n",
    "    \"delta_masscart\": np.linspace(MASSCART_MIN-masscart, MASSCART_MAX-masscart, 3),\n",
    "    \"delta_masspole\": np.linspace(MASSPOLE_MIN-masspole, MASSPOLE_MAX-masspole, 3),\n",
    "    \"delta_length\": np.linspace(LENGTH_MIN-length, LENGTH_MAX-length, 3),\n",
    "    \"delta_force_mag\": np.linspace(FORCE_MAG_MIN-force_mag, FORCE_MAG_MAX-force_mag, 3),\n",
    "}\n",
    "# action space compress: change only one action at a time. compress from 3^5 --> 3*5\n",
    "\n",
    "\n",
    "# Define the observation space for the adversary (assuming discrete spaces)\n",
    "observation_space = {\n",
    "    \"gravity\": np.linspace(GRAVITY_MIN, GRAVITY_MAX, 3),\n",
    "    \"masscart\": np.linspace(MASSCART_MIN, MASSCART_MAX, 3),\n",
    "    \"masspole\": np.linspace(MASSPOLE_MIN, MASSPOLE_MAX, 3),\n",
    "    \"length\": np.linspace(LENGTH_MIN, LENGTH_MAX, 3),\n",
    "    \"force_mag\": np.linspace(FORCE_MAG_MIN, FORCE_MAG_MAX, 3),\n",
    "}\n",
    "\n",
    "action_space_variables = action_space.keys()\n",
    "num_delta_per_variable = 3\n",
    "\n",
    "get_action_from_index_map = {\n",
    "    variable_index * num_delta_per_variable + change_index: (variable_index, change_index,variable)\n",
    "    for variable_index, variable in enumerate(action_space_variables)\n",
    "    for change_index in range(num_delta_per_variable)\n",
    "}\n",
    "\n",
    "def get_action_index(variable_index, change_index):\n",
    "    \"\"\"\n",
    "    Convert a variable index and a change index to a single action index.\n",
    "    There are 5 variables and 3 changes, so the total number of actions is 15.\n",
    "    \"\"\"\n",
    "    return variable_index * 3 + change_index\n",
    "\n",
    "def get_action_from_index(action_index):\n",
    "    \"\"\"\n",
    "    Convert a single action index back to the variable index and change index.\n",
    "    \"\"\"\n",
    "    variable_index,index,variable_name=get_action_from_index_map[action_index]\n",
    "    delta_value=action_space[variable_name][index]\n",
    "    \n",
    "    return variable_name, delta_value\n",
    "\n",
    "def get_observation_index(observation):\n",
    "    \"\"\"\n",
    "    Convert an observation (a list of indices representing the state of each variable)\n",
    "    to a single index using base-3 arithmetic.\n",
    "    \"\"\"\n",
    "    base = 3\n",
    "    observation_index = 0\n",
    "    for i, obs in enumerate(observation):\n",
    "        observation_index += obs * (base ** i)\n",
    "    return observation_index\n",
    "\n",
    "def get_observation_from_index(index):\n",
    "    \"\"\"\n",
    "    Convert a single observation index back to the observation state.\n",
    "    \"\"\"\n",
    "    observation = []\n",
    "    base = 3\n",
    "    for i in range(5):\n",
    "        observation.append(index % base)\n",
    "        index //= base\n",
    "    return observation[::-1]  # Reverse it because the last element corresponds to the highest place value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Parameters for Q-learning\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.99\n",
    "epsilon = 0.1  # exploration rate\n",
    "num_episodes = 100\n",
    "init_scale=0.01\n",
    "\n",
    "\n",
    "# Initialize Q-table with random noise\n",
    "statespacesize = np.prod([len(observation_space[key]) for key in observation_space])\n",
    "actionspacesize = 15\n",
    "Q = np.random.randn(statespacesize, actionspacesize) * init_scale\n",
    "log_info(\"Q matrix initialised with size :{}\",Q.shape)\n",
    "\n",
    "\n",
    "\n",
    "adv_rewards=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_history={}\n",
    "for episode in range(num_episodes):\n",
    "    counter=0\n",
    "    print(episode)\n",
    "    gravity, masscart, masspole, length, force_mag = env.get_params()  # For the adversary\n",
    "    state_index = get_observation_index([\n",
    "        np.digitize([gravity], observation_space['gravity'])[0] - 1,\n",
    "        np.digitize([masscart], observation_space['masscart'])[0] - 1,\n",
    "        np.digitize([masspole], observation_space['masspole'])[0] - 1,\n",
    "        np.digitize([length], observation_space['length'])[0] - 1,\n",
    "        np.digitize([force_mag], observation_space['force_mag'])[0] - 1,\n",
    "    ])\n",
    "    state_history[episode]=[]\n",
    "    doneAdversary = False\n",
    "\n",
    "    while not doneAdversary and counter<1000:\n",
    "        \n",
    "        action = {param: 0 for param in action_space.keys()}  # Initialize all actions to 'no change'\n",
    "        action_index=-1\n",
    "        \n",
    "        # Choose action from state using policy derived from Q (e.g., Îµ-greedy)\n",
    "        if np.random.rand() < epsilon:\n",
    "            # # Choose a random action for this parameter\n",
    "            # action_value = np.random.choice(action_space[parameter_to_modify])\n",
    "            action_index=np.random.randint(actionspacesize)\n",
    "            parameter_modify,action_value=get_action_from_index(action_index)\n",
    "            \n",
    "\n",
    "            # Create an action dictionary with the chosen modification\n",
    "            action[parameter_modify] = action_value\n",
    "            \n",
    "        else:\n",
    "            action_index = np.argmax(Q[state_index])  # Exploit learned values\n",
    "            # print(action_index,state_index,Q[state_index])\n",
    "            variable, delta = get_action_from_index(action_index)\n",
    "            action[variable] = delta\n",
    "            \n",
    "        \n",
    "        # Take the action and modify the environment parameters\n",
    "        env.init_params(**action)\n",
    "        env.reset()\n",
    "\n",
    "        doneAgent = False\n",
    "        total_reward = 0\n",
    "        obs,_= env.reset()  # Move the reset outside of the while loop\n",
    "        while not doneAgent and counter<1000:\n",
    "            # Evaluate the PPO agent's performance with the modified parameters\n",
    "            obs = obs.reshape(1, -1)\n",
    "            a, _states = ppo_agent.predict(obs, deterministic=True)\n",
    "            temp= env.step(a.item())\n",
    "            obs, r, doneAgent,info,_=temp\n",
    "            \n",
    "            total_reward += r\n",
    "        \n",
    "        counter=counter+1\n",
    "\n",
    "        # Adversary's reward is the negative of the PPO agent's total reward\n",
    "        outer_reward = -total_reward\n",
    "        # next_params = env.get_params()\n",
    "        next_g, next_m, next_mp, next_l, next_fmg = env.get_params()\n",
    "        next_state_index = get_observation_index([\n",
    "            np.digitize([next_g], observation_space['gravity'])[0] - 1,\n",
    "            np.digitize([next_m], observation_space['masscart'])[0] - 1,\n",
    "            np.digitize([next_mp], observation_space['masspole'])[0] - 1,\n",
    "            np.digitize([next_l], observation_space['length'])[0] - 1,\n",
    "            np.digitize([next_fmg], observation_space['force_mag'])[0] - 1,\n",
    "        ])\n",
    "        \n",
    "        state_history[episode].append((state_index,action_index))\n",
    "        # Update Q-table using the Q-learning algorithm\n",
    "        Q[state_index, action_index] += learning_rate * (outer_reward + discount_factor * np.max(Q[next_state_index]) - Q[state_index, action_index])\n",
    "        hm = sn.heatmap(data = Q,annot=True) \n",
    "        plt.show()\n",
    "        print(\"Outer reward:\",outer_reward)\n",
    "        adv_rewards.append(outer_reward)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(ppo_agent, env, n_eval_episodes=100, determinsitic=False)\n",
    "print(f\"Mean_reward for the PPO agent:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "env.close()\n",
    "mean_adv_reward=np.mean(np.sum(adv_rewards))\n",
    "print(f\"Mean reward of the adversary: {mean_adv_reward:.2f} - Num episodes: {num_episodes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.maximum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robustness-cartpole",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
